{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def load_and_concatenate(files):\n",
    "    # 각 파일별로 DataFrame을 생성하고, 파일명을 Source File 열로 추가\n",
    "    dfs = [pd.read_excel(file).assign(**{'Source File': file}) for file in files]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "def extract_dates_from_filenames(filenames, dataframe):\n",
    "    dates = []\n",
    "    for filename in filenames:\n",
    "        match = re.search(r'\\d{8}', filename)  \n",
    "        if match:\n",
    "            date = datetime.strptime(match.group(), '%Y%m%d')\n",
    "        else:\n",
    "            date = np.nan\n",
    "        dates.extend([date] * len(dataframe[dataframe['Source File'] == filename]))\n",
    "    return dates\n",
    "\n",
    "train_files = ['dataset/updated_trending_videos_final_20240806.xlsx', 'dataset/updated_trending_videos_final_20240811.xlsx', 'dataset/updated_trending_videos_final_20240815.xlsx', 'dataset/updated_trending_videos_final_20240819.xlsx']\n",
    "valid_files = ['dataset/updated_trending_videos_final_20240807.xlsx', 'dataset/updated_trending_videos_final_20240812.xlsx']\n",
    "test_files = ['dataset/updated_trending_videos_final_20240810.xlsx', 'dataset/updated_trending_videos_final_20240816.xlsx']\n",
    "\n",
    "\n",
    "train_df = load_and_concatenate(train_files)\n",
    "valid_df = load_and_concatenate(valid_files)\n",
    "test_df = load_and_concatenate(test_files)\n",
    "\n",
    "train_df['Upload Date'] = extract_dates_from_filenames(train_files, train_df)\n",
    "valid_df['Upload Date'] = extract_dates_from_filenames(valid_files, valid_df)\n",
    "test_df['Upload Date'] = extract_dates_from_filenames(test_files, test_df)\n",
    "\n",
    "\n",
    "# Duration Category: 비디오 길이 범주화\n",
    "def categorize_duration(duration):\n",
    "    if duration < 300:  # 5분 미만\n",
    "        return 'Short'\n",
    "    elif duration < 1200:  # 5분 이상 20분 미만\n",
    "        return 'Medium'\n",
    "    else:  # 20분 이상\n",
    "        return 'Long'\n",
    "\n",
    "train_df['Duration Category'] = train_df['Duration'].apply(categorize_duration)\n",
    "valid_df['Duration Category'] = valid_df['Duration'].apply(categorize_duration)\n",
    "test_df['Duration Category'] = test_df['Duration'].apply(categorize_duration)\n",
    "\n",
    "# Duration to Views Ratio: 비디오 길이를 조회수로 나눈 비율\n",
    "train_df['Duration to Views Ratio'] = train_df['Duration'] / train_df['Views']\n",
    "valid_df['Duration to Views Ratio'] = valid_df['Duration'] / valid_df['Views']\n",
    "test_df['Duration to Views Ratio'] = test_df['Duration'] / test_df['Views']\n",
    "\n",
    "# Likes to Views Ratio: 조회수 대비 좋아요의 비율\n",
    "train_df['Likes to Views Ratio'] = train_df['Likes'] / train_df['Views']\n",
    "valid_df['Likes to Views Ratio'] = valid_df['Likes'] / valid_df['Views']\n",
    "test_df['Likes to Views Ratio'] = test_df['Likes'] / test_df['Views']\n",
    "\n",
    "# Weekday/Weekend: 비디오가 평일에 업로드되었는지, 주말에 업로드되었는지 여부\n",
    "def get_weekday_or_weekend(upload_date):\n",
    "    if pd.isna(upload_date):\n",
    "        return np.nan\n",
    "    return 'Weekend' if upload_date.weekday() >= 5 else 'Weekday'\n",
    "\n",
    "train_df['Weekday/Weekend'] = train_df['Upload Date'].apply(get_weekday_or_weekend)\n",
    "valid_df['Weekday/Weekend'] = valid_df['Upload Date'].apply(get_weekday_or_weekend)\n",
    "test_df['Weekday/Weekend'] = test_df['Upload Date'].apply(get_weekday_or_weekend)\n",
    "\n",
    "# Day of Week: 업로드된 요일\n",
    "train_df['Day of Week'] = train_df['Upload Date'].dt.day_name()\n",
    "valid_df['Day of Week'] = valid_df['Upload Date'].dt.day_name()\n",
    "test_df['Day of Week'] = test_df['Upload Date'].dt.day_name()\n",
    "\n",
    "train_df = train_df.drop(columns=['Source File'])\n",
    "valid_df = valid_df.drop(columns=['Source File'])\n",
    "test_df = test_df.drop(columns=['Source File'])\n",
    "\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(valid_df.head())\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_df['Views'].dropna(), bins=20, edgecolor='k')\n",
    "plt.title('Distribution of Views')\n",
    "plt.xlabel('Views')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR 방법으로 이상치 제거\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "cleaned_train_df = remove_outliers(train_df, 'Views')\n",
    "\n",
    "# 히스토그램 생성\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(cleaned_train_df['Views'], bins=20, edgecolor='k')\n",
    "plt.title('Distribution of Views for train data(Outliers Removed)')\n",
    "plt.xlabel('Views')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_valid_df = remove_outliers(valid_df, 'Views')\n",
    "\n",
    "# 히스토그램 생성\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(cleaned_valid_df['Views'], bins=20, edgecolor='k')\n",
    "plt.title('Distribution of Views for valid data(Outliers Removed)')\n",
    "plt.xlabel('Views')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test_df = remove_outliers(test_df, 'Views')\n",
    "\n",
    "# 히스토그램 생성\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(cleaned_test_df['Views'], bins=20, edgecolor='k')\n",
    "plt.title('Distribution of Views for test data(Outliers Removed)')\n",
    "plt.xlabel('Views')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = cleaned_train_df.sort_values('Views')  # 조회수 기준으로 오름차순 정렬\n",
    "train_df = cleaned_train_df.drop_duplicates(subset='Video ID', keep='first')  # 중복된 Video ID에서 첫 번째(조회수가 낮은 것)만 남김\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = cleaned_valid_df.sort_values('Views')  # 조회수 기준으로 오름차순 정렬\n",
    "valid_df = cleaned_valid_df.drop_duplicates(subset='Video ID', keep='first')  # 중복된 Video ID에서 첫 번째(조회수가 낮은 것)만 남김\n",
    "\n",
    "valid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = cleaned_test_df.sort_values('Views')  # 조회수 기준으로 오름차순 정렬\n",
    "test_df = cleaned_test_df.drop_duplicates(subset='Video ID', keep='first')  # 중복된 Video ID에서 첫 번째(조회수가 낮은 것)만 남김\n",
    "\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 변수(Video ID, Title)는 큰 영향을 안 미칠 것으로 보아 drop\n",
    "drop_cols = ['Video ID', 'Title','Upload Date']\n",
    "# train_df_num = augmented_train_df.drop(columns=drop_cols)\n",
    "# valid_df_num = augmented_valid_df.drop(columns=drop_cols)\n",
    "# test_df_num = augmented_test_df.drop(columns=drop_cols)\n",
    "\n",
    "train_df_num = train_df.drop(columns=drop_cols)\n",
    "valid_df_num = valid_df.drop(columns=drop_cols)\n",
    "test_df_num = test_df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_num.dropna(axis=0, inplace=True)\n",
    "valid_df_num.dropna(axis=0, inplace=True)\n",
    "test_df_num.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 수치형 변수 목록\n",
    "numerical_cols = ['Likes', 'Duration', 'Duration to Views Ratio', 'Likes to Views Ratio']\n",
    "\n",
    "# 범주형 변수들을 원핫 인코딩\n",
    "train_df_encoded = pd.get_dummies(train_df_num, columns=['Category Name', 'Duration Category', 'Weekday/Weekend', 'Day of Week'])\n",
    "valid_df_encoded = pd.get_dummies(valid_df_num, columns=['Category Name', 'Duration Category', 'Weekday/Weekend', 'Day of Week'])\n",
    "test_df_encoded = pd.get_dummies(test_df_num, columns=['Category Name', 'Duration Category', 'Weekday/Weekend', 'Day of Week'])\n",
    "\n",
    "# train, valid, test 데이터셋의 열을 일관되게 유지\n",
    "valid_df_encoded = valid_df_encoded.reindex(columns=train_df_encoded.columns, fill_value=0)\n",
    "test_df_encoded = test_df_encoded.reindex(columns=train_df_encoded.columns, fill_value=0)\n",
    "\n",
    "# StandardScaler를 사용하여 수치형 변수만 스케일링\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 수치형 변수만 스케일링하여 새로운 DataFrame 생성\n",
    "train_df_scaled = train_df_encoded.copy()\n",
    "train_df_scaled[numerical_cols] = scaler.fit_transform(train_df_encoded[numerical_cols])\n",
    "\n",
    "valid_df_scaled = valid_df_encoded.copy()\n",
    "valid_df_scaled[numerical_cols] = scaler.transform(valid_df_encoded[numerical_cols])\n",
    "\n",
    "test_df_scaled = test_df_encoded.copy()\n",
    "test_df_scaled[numerical_cols] = scaler.transform(test_df_encoded[numerical_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성(feature)과 타겟(target) 설정\n",
    "X_train = train_df_scaled.drop(columns=['Views'])\n",
    "y_train = train_df_scaled['Views']\n",
    "X_valid = valid_df_scaled.drop(columns=['Views'])\n",
    "y_valid = valid_df_scaled['Views']\n",
    "X_test = test_df_scaled.drop(columns=['Views'])\n",
    "y_test = test_df_scaled['Views']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 칼럼만 사용\n",
    "common_cols = X_train.columns.intersection(X_valid.columns)\n",
    "\n",
    "X_train_common = X_train[common_cols]\n",
    "X_valid_common = X_valid[common_cols]\n",
    "X_test_common = X_test[common_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(common_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가지표 출력을 위한 패키지 불러오기\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, r2_score \n",
    "\n",
    "# 평가지표를 출력하는 함수 만들기\n",
    "result_df = pd.DataFrame(columns = [\"MSE\", \"RMSE\", \"MAE\", \"SMAPE\", \"R2\"])\n",
    "\n",
    "def print_reg_result(model_name, data_type, valid_y, valid_pred):\n",
    "    MSE = mean_squared_error(valid_y, valid_pred)\n",
    "    RMSE = root_mean_squared_error(valid_y, valid_pred)\n",
    "    MAE = mean_absolute_error(valid_y, valid_pred)\n",
    "    SMAPE = np.mean((np.abs(valid_y - valid_pred))/(np.abs(valid_y) + np.abs(valid_pred)))*100\n",
    "    R2 = r2_score(valid_y, valid_pred)\n",
    "    result_df.loc[len(result_df)] = [model_name, data_type, MSE, RMSE, MAE, SMAPE, R2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트리 기반의 회귀 알고리즘 패키지 불러오기\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 새로운 하이퍼파라미터로 모델 선언하기\n",
    "RF_reg = RandomForestRegressor(max_depth = 36, max_features='sqrt', min_samples_leaf=2, min_samples_split=3, n_estimators=322)\n",
    "ET_reg = ExtraTreesRegressor(max_depth=30, max_features='sqrt', min_samples_leaf=1, min_samples_split=5, n_estimators=416)\n",
    "CAT_reg = CatBoostRegressor(border_count=124, depth=4, iterations=136, l2_leaf_reg=8, learning_rate=0.1092250914011541)\n",
    "LGBM_reg = LGBMRegressor(learning_rate = 0.034478254120072105, max_depth = 16, min_child_samples=4, n_estimators=222, num_leaves=39, subsample=0.8599855723111061)\n",
    "XGB_reg = XGBRegressor(colsample_bytree=0.6464290562027665, learning_rate=0.023800792606525824, max_depth=3, min_child_weight=4, n_estimators=253, subsample=0.6068644407327001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [('Random Forest', RF_reg),\n",
    "              ('Extra Trees', ET_reg),\n",
    "              ('Cat Boost', CAT_reg),\n",
    "              ('Light GBM', LGBM_reg),\n",
    "              ('XG Boost', XGB_reg)\n",
    "              ]\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(columns=[\"Model\", \"Data Type\", \"MSE\", \"RMSE\", \"MAE\", \"SMAPE\", \"R2\"])\n",
    "\n",
    "for model_name, model in model_list:\n",
    "    \n",
    "    # Fit the model on the entire training data\n",
    "    model.fit(X_train_common, y_train)\n",
    "    \n",
    "    # Predict on validation and test data\n",
    "    train_pred = model.predict(X_train_common)\n",
    "    valid_pred = model.predict(X_valid_common)\n",
    "    test_pred = model.predict(X_test_common)\n",
    "    \n",
    "    # Output the evaluation metrics\n",
    "    print_reg_result(model_name, 'Train', y_train, train_pred)\n",
    "    print_reg_result(model_name, 'Validation', y_valid, valid_pred)\n",
    "    print_reg_result(model_name, 'Test', y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest 모델의 변수 중요도 구하기\n",
    "train_pred = XGB_reg.predict(X_train_common)\n",
    "valid_pred = XGB_reg.predict(X_valid_common)\n",
    "test_pred = XGB_reg.predict(X_test_common)\n",
    "\n",
    "feature_imp_df = pd.DataFrame(XGB_reg.feature_importances_, index = X_train_common.columns)\n",
    "feature_imp_df.reset_index(inplace = True)\n",
    "feature_imp_df.columns = [\"Youtube Factor\", \"Feature Importance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 중요도가 높은 순서대로 공공도서관 요인 정렬하기\n",
    "feature_imp_df = feature_imp_df.sort_values(\"Feature Importance\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 중요도의 누적합을 담은 열 추가하기\n",
    "feature_imp_df[\"Feature Importance\"] = feature_imp_df[\"Feature Importance\"]/feature_imp_df[\"Feature Importance\"].sum()\n",
    "cum_feature_imp = feature_imp_df[\"Feature Importance\"].cumsum()\n",
    "feature_imp_df.insert(2, \"Cumsum of Feature Importance\", cum_feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 중요도의 누적합 계산\n",
    "feature_imp_df[\"Cumsum of Feature Importance\"] = feature_imp_df[\"Feature Importance\"].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임의 인덱스 초기화하기\n",
    "feature_imp_df.reset_index(inplace = True)\n",
    "feature_imp_df.drop(columns = \"index\", inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 중요도에 대한 최종 데이터프레임 출력하기\n",
    "feature_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 변수의 상대적 중요도 시각화하기 (Feature Importance Plot)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 상위 15개 특성만 선택하기\n",
    "top_20_features = feature_imp_df.nlargest(20, 'Feature Importance')\n",
    "\n",
    "# 시각화하기\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "sns.barplot(y=top_20_features[\"Youtube Factor\"], x=top_20_features[\"Feature Importance\"], palette=\"YlGnBu\")\n",
    "plt.title(\"Top 20 Feature Importance of XG Boost\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Plot & Force Plot 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(RF_reg)\n",
    "shap_values = explainer.shap_values(X_test_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, max_display=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values.values[6], X_test.iloc[0], matplotlib=True, figsize=(45, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(ET_reg)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, max_display=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values.values[6], X_test.iloc[0], matplotlib=True, figsize=(45, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(CAT_reg)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, max_display=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values.values[6], X_test.iloc[0], matplotlib=True, figsize=(45, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(LGBM_reg)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, max_display=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values.values[6], X_test.iloc[0], matplotlib=True, figsize=(45, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(XGB_reg)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, max_display=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values.values[6], X_test.iloc[0], matplotlib=True, figsize=(45, 3))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jw_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
